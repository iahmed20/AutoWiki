# -*- coding: utf-8 -*-
"""STORMalgo.ipynb

Automatically generated by Colab.
"""

#!pip install Wikipedia-API

#!pip install beautifulsoup4

#!pip install openai

import os
import openai
from google.colab import userdata
import requests
from bs4 import BeautifulSoup
import wikipediaapi
import nltk
from dotenv import load_dotenv

load_dotenv()


user = os.getenv("USER_AGENT")
openai.api_key = os.getenv("OPEN_AI_API")
wiki_wiki = wikipediaapi.Wikipedia(user_agent=user, language='en')


def get_related_topics(user_input):

  response = openai.chat.completions.create(
      model="gpt-4",
      messages=[
          {"role": "system", "content": "You are a wikipedia researcher"},
          {"role": "user", "content": f"“Given the Wikipedia article {user_input}, what are 10 other Wikipedia article titles that are strongly related and would provide additional perspectives on {user_input}? Exclude direct subtopics (like applications, ethics, agriculture, medicine) and focus on distinct but relevant scientific, technological, or social topics that would help expand an article on {user_input}. Return the final result as a valid Python list of strings and nothing else"},
          ]
  )

  return response.choices[0].message.content

def identify_perspectives(tocs):
  response = openai.chat.completions.create(
      model="gpt-4",
      messages=[
          {"role": "system", "content": "You are a wikipedia researcher"},
          {"role": "user", "content": f"You are generating perspectives (personas) for writing a comprehensive Wikipedia article about CRISPR. Task: 1. Read across the section titles and infer what types of experts or stakeholders would naturally speak about these topics. 2. Each perspective should be expressed as a role. 3. Remove duplicates or overly narrow personas. 4. Ensure diversity across science, medicine, agriculture, computation, ethics, and regulation. 5. Return the final result as a valid Python list of strings and nothing else. Here are the Wikipedia article sections you should analyze: {tocs}"},
          ]
  )


  response2 = openai.chat.completions.create(
      model="gpt-4",
      messages=[
          {"role": "system", "content": "You are a wikipedia researcher"},
          {"role": "user", "content": f"You are curating perspectives (personas) to contribute to a Wikipedia article about CRISPR. Input: {response.choices[0].message.content}] Task: 1. Remove duplicates and overly narrow roles. 2. Select only 3–5 perspectives that, together, provide a broad understanding of CRISPR (covering science, medicine, agriculture, and ethics/regulation). 3. Return the final list as a valid Python list of strings and nothing else."},
          ]
  )



  return response2.choices[0].message.content



#openai prompt, or use dspy w/openai: generate a list of topics, and then run the code down there for each article if there is one. If not then skip.

def get_toc(response):
  # Source: get_wiki_page_title_and_toc
  # Author: shaoyijia
  # Published: Apr 23, 2024 | Retrieved: Aug 18, 2025
  # Availability: https://github.com/stanford-oval/storm/blob/main/knowledge_storm/storm_wiki/modules/persona_generator.py
  # License: Stanford OVAL (check repo for details)
  #
  # Adapted code below:

  # Get the main title from the first h1 tag

  list_of_topics = response.choices[0].message.content

  toc = ""
  for topic in list_of_topics:
    page_py = wiki_wiki.page(topic)
    if not page_py.exists():
      toc_noWiki()
      continue

    page = requests.get(page_py.fullurl)
    soup = BeautifulSoup(page.content, "html.parser")
    main_title = soup.find("h1").text.replace("[edit]", "").strip().replace("\xa0", " ")


    levels = []
    #added in
    excluded_sections = {
        "Contents",
        "See also",
        "Notes",
        "References",
        "External links",
    }

    # Start processing from h2 to exclude the main title from TOC
    for header in soup.find_all(["h2", "h3", "h4", "h5", "h6"]):
        level = int(
            header.name[1]

        )  # Extract the numeric part of the header tag (e.g., '2' from 'h2')
        section_title = header.text.replace("[edit]", "").strip().replace("\xa0", " ")
        if section_title in excluded_sections:
            continue

        while levels and level <= levels[-1]:
            levels.pop()
        levels.append(level)

        indentation = "  " * (len(levels) - 1)
        toc += f"{indentation}{section_title}\n"

    # ---End Code Snippet---
  return toc

def toc_noWiki():
  #runs if there is no wiki page returned.
  pass

def main():
  user_input = input("Enter your Wiki Topic: ")
  response = get_related_topics(user_input)
  table_of_contents = get_toc(response)
  list_of_perspectives = identify_perspectives(table_of_contents)
  list_of_perspectives.append("Basic Fact Researcher")
  #simulate_conversation()

if __name__ == "__main__":
    main()

