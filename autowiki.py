# -*- coding: utf-8 -*-
"""AutoWiki.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KJua_ravo4v3t6XkYB2GgmJw1V2Bjzf
"""

!pip install requests beautifulsoup4 duckduckgo-search langchain chromadb tiktoken
!pip install transformers
!pip install langchain-community
!huggingface-cli login
!pip install bitsandbytes

from duckduckgo_search import DDGS #all necessary imports
import requests
from bs4 import BeautifulSoup
import json, time
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
import json

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from transformers import pipeline, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer
from langchain_community.llms import HuggingFacePipeline
import torch
import chromadb
import gc

def crawl(topic, max_results=10): #crawls web, duckduckgo search, top 10 pages
    results = DDGS().text(topic, max_results=max_results)
    crawled = []

    for r in results:
        try:
            page = requests.get(r['href'], timeout=10)
            soup = BeautifulSoup(page.text, 'html.parser')
            text = ' '.join(p.get_text() for p in soup.find_all('p'))
            crawled.append({'title': r['title'], 'url': r['href'], 'text': text[:10000]})
            print(f"Fetched: {r['title']}")
        except:
            print(f"Failed to fetch: {r['href']}")
        time.sleep(1)

    output_dir = 'data'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    file_path = os.path.join(output_dir, 'crawled.json')

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(crawled, f, ensure_ascii=False, indent=2)

def chunk_documents(input_path='data/crawled.json', output_path='data/chunks.json'):
    with open(input_path, 'r', encoding='utf-8') as f:
        docs = json.load(f)

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = []

    for doc in docs:
        text_chunks = splitter.split_text(doc['text'])
        for chunk in text_chunks:
            chunks.append({"title": doc['title'], "url": doc['url'], "content": chunk})

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)

def embed_and_index(chunk_path='data/chunks.json'):
        with open(chunk_path, 'r', encoding='utf-8') as f:
            chunks = json.load(f)

        client = chromadb.Client()

        try:
            client.delete_collection("wikipe")
            print("Deleted existing collection 'wikipe'")
        except:
            pass

        collection = client.create_collection("wikipe")
        embedder = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

        batch_size = 10 # ram limitation

        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            texts = [chunk['content'] for chunk in batch_chunks]
            metadata = [{"title": chunk['title'], "url": chunk['url']} for chunk in batch_chunks]

            ids = [str(j) for j in range(i, i + len(batch_chunks))]

            # Embed
            embeddings = embedder.embed_documents(texts)

            collection.add(
                documents=texts,
                metadatas=metadata,
                embeddings=embeddings,
                ids=ids
            )
            print(f"Indexed batch {i//batch_size + 1}")
            del batch_chunks, texts, metadata, embeddings, ids
            gc.collect() # free memory

        #print(f"Indexed {len(chunks)} chunks.")

def retrieve_and_generate(topic, sections):
    client = chromadb.Client()
    try:
        collection = client.get_collection("wikipe")
    except:
        print("Error: Collection 'wikipe' not found")
        return

    quantization_config = BitsAndBytesConfig(
            load_in_8bit=True
        )
    embedder = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")



    device = "cuda" if torch.cuda.is_available() else "cpu"
    #print(f"Using device: {device}")

    model_id = "meta-llama/Meta-Llama-3-8B"
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token


    # Initialize the pipeline with the loaded model and tokenizer
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16, # Add this line
        max_new_tokens=500,
        return_full_text=False,
        #repeat_penalty = 1.4 #cause issues
        # Increased max_new_tokens for more substantial text
    )

    # Wrap the pipeline in HuggingFacePipeline
    llm = HuggingFacePipeline(pipeline=pipe)

    article = f"# {topic}\n\n"

    for section in sections:
        query = f"{topic} {section}"
        query_emb = embedder.embed_query(query)

        results = collection.query(query_embeddings=[query_emb], n_results=5)
        # Ensure that results['documents'] is not empty before joining
        if results and results.get('documents') and results['documents'][0]:
            context = "\n\n".join(results['documents'][0])
        else:
            context = "No relevant context for this section."

        prompt = ChatPromptTemplate.from_template(
            f"Write a Wikipedia-style '{section}' section for '{topic}' using this context:\n{context}\n\n"
            "Maintain a neutral, encyclopedic style, clearly structure the content with substantial detail, and do not repeat this prompt"
        )


        prompt_text = prompt.format(section=section, topic=topic, context=context)


        response = llm.invoke(prompt_text)

        generated_text = response

        article += f"## {section}\n\n{generated_text}\n\n" # concat sections

    with open('article_output.md', 'w', encoding='utf-8') as f:
        f.write(article)

    print("article generated!")

if __name__ == '__main__':
    query = input("SEARCH: ")
    crawl(query)
    chunk_documents()
    embed_and_index()
    sections = ["Summary", "History", "Applications", "Society and Culture", "References"] #typical Wiki sections
    retrieve_and_generate(query, sections)